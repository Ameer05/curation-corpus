{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning BART for abstractive text summarisation with fastai2\n",
    "\n",
    "A great thing about working in NLP at the moment is being able to park a hard problem for a few weeks and discovering the community making massive amounts of progress on your behalf. I used to be overwhelmed by the challenge of just training a summarisation model to generate plausible looking text without burning through tonnes of cash on GPUs. Then [BertExtAbs](../finetuning-bertsumextabs) came along and solved that problem. Unfortunately, it still gernerated incoherent sentences sometimes and had a habit of confusing entities in an article. You certainly couldn't trust it to convey the facts of an article reliably.\n",
    "\n",
    "Enter BART (Bidirectional and Auto-Regressive Transformers). Here we have a model that generates staggeringly good summaries and has a wonderful implementation from Sam Shleifer at HuggingFace. It's still a work in progress, but after digging around in the Transformers pull requests and with help from [Morgan McGuire's FastHugs notebook](https://github.com/morganmcg1/fasthugs) I have put together this notebook for fine-tuning BART and generating summaries. Feedback welcome!\n",
    "\n",
    "I should mention that this a big model requiring big inputs. For fine-tuning I've been able to get a batch size of 4 and a maximum sequence length of 512 on an AWS P3.2xlarge (~Â£4 an hour).\n",
    "\n",
    "We begin with a bunch of imports and an args object for storing variables we will need. We'll be finetuning the model on the Curation Corpus of abstractive text summaries. We load it into a dataframe using Pandas. For more information about how to access this dataset for your own purposes please see our [article introducing the dataset](https://medium.com/curation-corporation/teaching-an-ai-to-abstract-a-new-dataset-for-abstractive-auto-summarisation-5227f546caa8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import logging\n",
    "logging.getLogger().setLevel(100)\n",
    "from fastprogress import progress_bar\n",
    "from fastai2.basics import *\n",
    "from fastai2.data import *\n",
    "from fastai2.text.all import *\n",
    "from fastai2.callback.all import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import PreTrainedTokenizer, BartTokenizer, BartForConditionalGeneration, BartConfig \n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully we will be able to increase our batch size and/or maximum sequence lengths when some pull requests to reduce the model's memory footprint get merged into the Transformers repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "args = Namespace(\n",
    "    batch_size=4,\n",
    "    max_seq_len=512,\n",
    "    data_path=\"../data/private_dataset.file\",\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), # ('cpu'),\n",
    "    stories_folder='../data/my_own_stories',\n",
    "    subset=None,\n",
    "    test_pct=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_feather(args.data_path).iloc[:args.subset]\n",
    "ds = ds[ds['summary'] != '']\n",
    "train_ds, test_ds = train_test_split(ds, test_size=args.test_pct, random_state=42)\n",
    "valid_ds, test_ds = train_test_split(test_ds, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass our data to the model in our fastai2 learner object we need a dataloader. To create a dataloader we need a Datasets object, batch size, and device type. To create a Datasets object, we have to pass a few things:\n",
    "- Our raw data which in our case is a Pandas dataframe\n",
    "- A list of transforms. Or to be more precise a list containing the list of transforms to perform on our inputs and a list of transforms to perform on our desired outputs. I've defined a transform below that encodes the text using the BART tokenizer. Mostly it will be the encodes class method that gets called by fastai2. However the decodes method can also be useful if you want to reverse the process.\n",
    "- We will also split our data into training and validation datasets here, using fastai2's RandomSplitter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('bart-large-cnn', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm still exploring whether it is necessary to pass any of the masks and other ids manually or if it is handled for us. Any advice here would be much appreciated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform(Transform):\n",
    "    def __init__(self, tokenizer:PreTrainedTokenizer, column:string):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.column = column\n",
    "        \n",
    "    def encodes(self, inp):  \n",
    "        tokenized = self.tokenizer.batch_encode_plus(\n",
    "            [list(inp[self.column])],\n",
    "            max_length=args.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return TensorText(tokenized['input_ids']).squeeze()\n",
    "        \n",
    "    def decodes(self, encoded):\n",
    "        decoded = [\n",
    "            self.tokenizer.decode(\n",
    "                o, \n",
    "                skip_special_tokens=True, \n",
    "                clean_up_tokenization_spaces=False\n",
    "            ) for o in encoded\n",
    "        ]\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms = [DataTransform(tokenizer, column='text')]\n",
    "y_tfms = [DataTransform(tokenizer, column='summary')]\n",
    "dss = Datasets(\n",
    "    train_ds, \n",
    "    tfms=[x_tfms, y_tfms], \n",
    "    splits=RandomSplitter(valid_pct=0.1)(range(train_ds.shape[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dss.dataloaders(bs=args.batch_size, device=args.device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function lets us choose between loading the model architecture with Facebook's pretrained weights, the model architecture with our own weights stored locally, or the model architecture with no pretraining at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_model(config, pretrained=False, path=None): \n",
    "    if pretrained:    \n",
    "        if path:\n",
    "            model = BartForConditionalGeneration.from_pretrained(\n",
    "                \"bart-large-cnn\", \n",
    "                state_dict=torch.load(path, map_location=torch.device(args.device)), \n",
    "                config=config\n",
    "            )\n",
    "        else: \n",
    "            model = BartForConditionalGeneration.from_pretrained(\"bart-large-cnn\", config=config)\n",
    "    else:\n",
    "        model = BartForConditionalGeneration()\n",
    "\n",
    "    return model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will return a lot of different things, but we only want the weights to calculate the loss when training, so we will wrap the model in this class to control what gets passed to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastaiWrapper(Module):\n",
    "    def __init__(self):\n",
    "        self.config = BartConfig(vocab_size=50264, output_past=True)\n",
    "        self.bart = load_hf_model(config=self.config, pretrained=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.bart(x)[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of seq2seq tasks as a series of attempts to categorise which word should come next. Cross entropy loss is a pretty good loss function for this use case. We want to normalise it by how many non padding words are in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarisationLoss(Module):\n",
    "    def __init__(self):\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        x = F.log_softmax(output, dim=-1)\n",
    "        norm = (target != 1).data.sum()\n",
    "        return self.criterion(x.contiguous().view(-1, x.size(-1)), target.contiguous().view(-1)) / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fine-tuning the model we start by just training the top layer(s). You can experiment by unfreezing layers further down in the decoder, and then (if you're feeling bold) then encoder. fastai2 provides an easy way to split the model up into groups with frozen or unfrozen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_splitter(model):\n",
    "    return [\n",
    "        params(model.bart.model.encoder), \n",
    "        params(model.bart.model.decoder.embed_tokens),\n",
    "        params(model.bart.model.decoder.embed_positions),\n",
    "        params(model.bart.model.decoder.layers),\n",
    "        params(model.bart.model.decoder.layernorm_embedding),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been experimenting with half precision training. In theory this will save a lot of memory. However, I find my loss quickly becomes a bunch of nans. This may be an issue with HuggingFace's implementation or it may be an issue with my code. I'll update if I work out how to get fp16() working. Do let me know if you have any ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls, \n",
    "    FastaiWrapper(), \n",
    "    loss_func=SummarisationLoss(), \n",
    "    opt_func=ranger,\n",
    "    splitter=bart_splitter\n",
    ")#.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been finding that the learning rate finder suggests values that are too high. Your mileage may vary though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_flat_cos(\n",
    "    1,\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do carry on unfreezing layers, you may find that you need to reduce your batch size to fit everything in memory. Also you should probably lower your learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.dls.train.bs = args.batch_size//2\n",
    "learn.dls.valid.bs = args.batch_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(\n",
    "    2,\n",
    "    lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is done we can export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('../models/fintuned_bart.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner('../models/fintuned_bart.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code for generating the summaries comes from [Sam Shleifer's example in the Transformers repository](https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/evaluate_cnn.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def generate_summaries(lns, out_file, batch_size=4):\n",
    "    dec = []\n",
    "    for batch in progress_bar(list(chunks(lns, batch_size))):\n",
    "        dct = tokenizer.batch_encode_plus(\n",
    "            batch, \n",
    "            max_length=1024, \n",
    "            return_tensors=\"pt\", \n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "        \n",
    "        summaries = learn.model.bart.to(args.device).generate(\n",
    "            input_ids=dct[\"input_ids\"].to(args.device),\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            max_length=142,\n",
    "            min_length=56,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "        \n",
    "        dec.extend([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries])\n",
    "        \n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2/2 00:08<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lns = [\" \" + x.rstrip() for x in list(test_ds['text'])[:8]]\n",
    "bart_sums = generate_summaries(lns, f'{args.stories_folder}/output.txt', batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPPO's first 5G smartphone has received 5G CE certification, paving the way for the company to commercially launch the device in Europe. The phone maker claims that it is the first multi-frequency, multi-mode and multi-EN-DC (which means dual connectivity for LTE and 5G) smartphone to be certified by CTC Advanced GmbH. OPPO inked a global patent license agreement with Ericsson, which covers the patent portfolios of both companies in 2G, 3G and 4G, as well as cooperation on device testing, customer engagements and a demonstration at MWC19.\n",
      "***************\n",
      "Streaming music from Amazon Music, TuneIn, iHeartRadio, and Pandora, with support for Spotify and SiriusXM available shortly. Using the Amazon Alexa App simply create groups of Echo devices and then simply ask Alexa to play on those devices. Amazon is excited to be working with leading brands on this offering, including Sonos, Bose, Sound United, and Samsung.\n",
      "***************\n",
      "LanzaTech uses anaerobic bacteria (originally found in rabbit droppings) to ferment waste emissions from industry. Facility has a capacity of 46,000 tons (16 million gallons) of ethanol per year. This ethanolâs performance in fuel blending applications is indistinguishable from sugar-derived ethanol. It meets all specifications of ASTM International D4806, the active standard for qualifying ethanol used in blending with gasoline for automotive engines. In addition, the ethanol meets the National Standard of the People's Republic of China GB 18350 for Denatured fuel ethanol.\n",
      "***************\n",
      "UK energy leaders are more worried about the risk of cyber attacks caused by the rise in digitisation of energy systems than other countries. The rapid expansion of solar, the smart meter roll-out and the introduction of half hourly metering has increased the level of digital reliance. Other key uncertainties that keep Britain's energy leaders awake at night were cohesion in the European Union post-Brexit, electric storage and commodity prices. Top action priorities were renewable energy, the global climate framework, energy efficiency and electricity prices.\n",
      "***************\n",
      "Coal use by utility companies has plummeted amid low natural gas prices. Ten years ago coal produced 50 percent of the nationâs power supply. California is the largest insurance market in the United States and sixth-largest in the world. Arch Coal Inc, the nation's second-largest U.S. coal miner, filed for bankruptcy protection earlier this month.\n",
      "***************\n",
      "China is building a vast DNA database with no appropriate privacy protection, rights activists warn. Ordinary citizens are being asked to have their blood drawn for a DNA sample, Human Rights Watch says. Vulnerable groups and minorities appear to be a particular target of the push. Xinjiang authorities are reported to have bought around $10bn (Â£7.7bn) in equipment to step up the collection and indexing of DNA.\n",
      "***************\n",
      "New rules ban people on any of the Balearic islands from renting out space in flats via websites such as Airbnb and Homeaway unless they first obtain a licence. They face fines of up to â¬400,000 if they break the law. Companies, including Airbnb, face a similar penalty for allowing clients to advertise without a valid licence number. âWhat the local government is doing is half-witted â this is a tourist island, nothing else makes money,â said Ray, a Briton who preferred not to give his surname.\n",
      "***************\n",
      "Suzlon Energy said it defaulted on a $172-million bond payment on Tuesday. Its shares fell more than 3% to close at Rs 4.6 on Tuesday on the BSE. Sources said the wind power equipment maker was in talks with several global private equity funds to sell a majority stake. Suzlon is struggling with debt of over Rs 11,000 crore ($1.6 billion)\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "for s in bart_sums[:8]:\n",
    "    print(s)\n",
    "    print(\"***************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py374",
   "language": "python",
   "name": "py374"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
